{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee97738-08c9-49c7-a632-101c749f1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from voyager import Voyager\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "import copy\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a30b35-7324-4ed8-9504-094023a22f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_port = 25565\n",
    "if \"VOYAGER_MC_PORT\" in os.environ:\n",
    "    mc_port = os.environ[\"VOYAGER_MC_PORT\"]\n",
    "\n",
    "\n",
    "if \"OPENAI_API_MODEL\" in os.environ:\n",
    "    model = os.environ[\"OPENAPI_API_MODEL\"]\n",
    "else:\n",
    "    model = \"meta-llama/llama-3.1-405b-instruct:free\"\n",
    "\n",
    "\n",
    "if \"OPENAI_API_BASE\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\" #openrouter\n",
    "    # os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:12434/engines/llama.cpp/v1\" #locally hosting llama\n",
    "    # os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:8000/v1\" #locally hosting vllm\n",
    "\n",
    "### SETUP THESE VARIABLES\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
    "\n",
    "debug_print = \"VOYAGER_DEBUG_PRINT\" in os.environ\n",
    "\n",
    "# uses os.environ[\"SERPER_API_KEY\"]\n",
    "serper = GoogleSerperAPIWrapper()\n",
    "\n",
    "request_timeout = 300 #model request_timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c13594-ecc6-41eb-b990-88fd0f359cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search(query: str) -> int:\n",
    "    \"\"\"Looks up information on the web, and gets a summary of the results. Use this if theres any information related to or about Minecraft that needs to be known.\n",
    "    \n",
    "    Args:\n",
    "        query: Lookup query\n",
    "    \"\"\"\n",
    "    if debug_print:\n",
    "        print(f\"searching up {query} on the web\")\n",
    "    result = serper.run(query)\n",
    "    if debug_print:\n",
    "        print(f\"web result was {result}\")\n",
    "    return result\n",
    "\n",
    "# @tool\n",
    "# def retrieve(query: str):\n",
    "#     \"\"\"Retrieve Minecraft related information related to the query. Use this whenever any Minecraft term comes up.\n",
    "    \n",
    "#     Args:\n",
    "#         query: What to find Minecraft information about.\n",
    "        \n",
    "#     Returns: Relavant Minecraft data to your query.\n",
    "#     \"\"\"\n",
    "#     retrieved_docs = vector_store.similarity_search_by_vector(embedding = embeddings.embed_query(query), k=3)\n",
    "#     serialized = \"\\n\\n\".join(\n",
    "#         # (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "#         (f\"Content: {doc.page_content}\")\n",
    "#         for doc in retrieved_docs\n",
    "#     )\n",
    "    \n",
    "#     if debug_print:\n",
    "#         print(f\"query was {query}, returned information was {serialized}\")\n",
    "#     return serialized\n",
    "\n",
    "thinking_pattern = r\"<think>.*<\\/think>\\n?\\n?\";\n",
    "def llm_invoker(llm, in_messages, retries = 3):\n",
    "    try:\n",
    "        if debug_print:\n",
    "            print(f\"=====================================SENT TO AI=====================================\")\n",
    "            print(str(in_messages))\n",
    "        messages = copy.deepcopy(in_messages)\n",
    "        result = llm.invoke(messages)\n",
    "\n",
    "        ### repeatedly checks for tool calls, calls them, and continues running the llm\n",
    "        while True:\n",
    "            all_satisfied = True\n",
    "            if debug_print:\n",
    "                print(f\"==============INITIAL RESULT==============\")\n",
    "                print(f\"{result}\")\n",
    "                print(f\"tools called: {result.tool_calls}\")\n",
    "            for tool_call in result.tool_calls:\n",
    "                all_satisfied = False\n",
    "                selected_tool = {\"search\": search, \"retrieve\": retrieve}[tool_call[\"name\"].lower()]\n",
    "                tool_msg = selected_tool.invoke(tool_call)\n",
    "                messages.append(tool_msg)\n",
    "            if all_satisfied:\n",
    "                break\n",
    "            result = llm.invoke(messages)\n",
    "        \n",
    "        if debug_print:\n",
    "            print(f\"=====================FINAL RESULT=====================\")\n",
    "            print(result.content)\n",
    "            print(f\"=====================================END OF AI=====================================\")\n",
    "        \n",
    "        result.content = re.sub(thinking_pattern, \"\", result.content,flags=re.DOTALL) # dirty fix for some hosters which keep the reasoning text in the content\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"LLM ERROR {e}\")\n",
    "        print(f\"{retries} RETRIES LEFT\")\n",
    "        if retries==0:\n",
    "            print(f\"NO RETRIES LEFT, RETURNING EMPTY\")\n",
    "            return AIMessage(\"\")\n",
    "        return llm_invoker(llm, in_messages, retries-1)\n",
    "\n",
    "def llm_preprocesser(llm):\n",
    "    return llm\n",
    "    # return llm.bind_tools([search]) # binding a tool to the llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678eac8f-a4bd-47cb-95ca-16bf419b0ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152dd87b-02f4-482b-bbee-193fda33e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_function = HuggingFaceEndpointEmbeddings( # huggingface example\n",
    "#     model=\"http://localhost:8080\", # if you're using Text Embeddings Interface locally, set it to the url\n",
    "#     huggingfacehub_api_token=\"\", # not needed if running locally\n",
    "#     request_timeout=request_timeout,\n",
    "# )\n",
    "# embedding_function = OpenAIEmbeddings(\n",
    "#     base_url = \"http://localhost:8080/v1\", # currently using llamacpp\n",
    "#     model=\"\", #unused if llamacpp\n",
    "#     request_timeout=request_timeout,\n",
    "# )\n",
    "\n",
    "embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=\"Qwen/Qwen3-Embedding-8B\",\n",
    "    provider=\"nebius\",\n",
    "    # huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    ")\n",
    "\n",
    "# use_pre_planning_prompt is to decide if it uses _no_pre_planning prompts (in voyager/prompts)\n",
    "# originally made for possibly switching between reasoning and non reasoning models, in the theory that\n",
    "# reasoning models shouldn't need to be pre-prompted to write out plans\n",
    "\n",
    "voyager = Voyager(\n",
    "    mc_port=mc_port,\n",
    "    \n",
    "    action_agent_model=llm_preprocesser(ChatOpenAI(\n",
    "        model=model,\n",
    "        request_timeout=request_timeout,\n",
    "    )),\n",
    "    action_agent_use_pre_planning_prompt = True,\n",
    "    \n",
    "    curriculum_agent_qa_model=llm_preprocesser(ChatOpenAI(\n",
    "        model=model,\n",
    "        request_timeout=request_timeout,\n",
    "    )),\n",
    "    curriculum_agent_qa_use_pre_planning_prompt = True,\n",
    "    \n",
    "    curriculum_agent_model=llm_preprocesser(ChatOpenAI(\n",
    "        model=model,\n",
    "        request_timeout=request_timeout,\n",
    "    )),\n",
    "    curriculum_agent_use_pre_planning_prompt = True,\n",
    "    \n",
    "    critic_agent_model=llm_preprocesser(ChatOpenAI(\n",
    "        model=model,\n",
    "        request_timeout=request_timeout,\n",
    "    )),\n",
    "    critic_agent_use_pre_planning_prompt = True,\n",
    "    \n",
    "    skill_manager_model=llm_preprocesser(ChatOpenAI(\n",
    "        model=model,\n",
    "        request_timeout=request_timeout,\n",
    "    )),\n",
    "    skill_manager_use_pre_planning_prompt = True,\n",
    "    \n",
    "    curriculum_agent_embedding_function=embedding_function,\n",
    "    skill_manager_embedding_function=embedding_function,\n",
    "    llm_invoker=llm_invoker,\n",
    "    resume=False, #set to True if you're resuming from a checkpoint, if an error occurs during startup when it's False, you may want to try removing the ckpt folder\n",
    ")\n",
    "\n",
    "# start lifelong learning\n",
    "voyager.learn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
